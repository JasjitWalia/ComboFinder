{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP3e5Ci1eQSqBUGcn7mdf2+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JasjitWalia/ComboFinder/blob/main/Advanced_Python.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Xto9b_cCHFTf"
      },
      "outputs": [],
      "source": [
        "#creating file with 1000 random strings\n",
        "import random\n",
        "import string\n",
        "\n",
        "with open(\"random_strings.txt\", \"w\") as file:\n",
        "    for _ in range(1000):\n",
        "        random_string = ''.join(random.choices(string.ascii_lowercase, k=10))\n",
        "        file.write(random_string + \"\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# file with multiple lines of random strings and a file size of 5 MB\n",
        "file_size_limit = 5 * 1024 * 1024  # 5 MB\n",
        "\n",
        "with open(\"random_strings_large.txt\", \"w\") as file:\n",
        "    file_size = 0\n",
        "    while file_size < file_size_limit:\n",
        "        random_string = ''.join(random.choices(string.ascii_lowercase, k=10))\n",
        "        line = random_string + \"\\n\"\n",
        "        line_size = len(line)\n",
        "        \n",
        "        if file_size + line_size <= file_size_limit:\n",
        "            file.write(line)\n",
        "            file_size += line_size\n",
        "        else:\n",
        "            break\n"
      ],
      "metadata": {
        "id": "eCYlfKXHHdqZ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create 10 files, each containing multiple lines of random strings and with a file size of 5 MB\n",
        "file_size_limit = 5 * 1024 * 1024  # 5 MB\n",
        "\n",
        "for file_index in range(10):\n",
        "    file_name = f\"random_strings_{file_index}.txt\"\n",
        "    with open(file_name, \"w\") as file:\n",
        "        file_size = 0\n",
        "        while file_size < file_size_limit:\n",
        "            random_string = ''.join(random.choices(string.ascii_lowercase, k=10))\n",
        "            line = random_string + \"\\n\"\n",
        "            line_size = len(line)\n",
        "\n",
        "            if file_size + line_size <= file_size_limit:\n",
        "                file.write(line)\n",
        "                file_size += line_size\n",
        "            else:\n",
        "                break\n"
      ],
      "metadata": {
        "id": "bJNGN967Ht_2"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#To create 5 files of sizes 1GB, 2GB, 3GB, 4GB, and 5GB, each containing multiple lines of random strings\n",
        "file_sizes = [1, 2, 3, 4, 5]  # in GB\n",
        "\n",
        "for i, size in enumerate(file_sizes, start=1):\n",
        "    file_size_limit = size * 1024 * 1024 * 1024  # Convert GB to bytes\n",
        "    file_name = f\"random_strings_{size}GB.txt\"\n",
        "\n",
        "    with open(file_name, \"w\") as file:\n",
        "        file_size = 0\n",
        "        while file_size < file_size_limit:\n",
        "            random_string = ''.join(random.choices(string.ascii_lowercase, k=10))\n",
        "            line = random_string + \"\\n\"\n",
        "            line_size = len(line)\n",
        "\n",
        "            if file_size + line_size <= file_size_limit:\n",
        "                file.write(line)\n",
        "                file_size += line_size\n",
        "            else:\n",
        "                break\n"
      ],
      "metadata": {
        "id": "irAHyd5GIIPg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#convert all the files created in Q4 into uppercase\n",
        "import os\n",
        "\n",
        "file_sizes = [1, 2, 3, 4, 5]  # in GB\n",
        "\n",
        "for size in file_sizes:\n",
        "    file_name = f\"random_strings_{size}GB.txt\"\n",
        "    output_file = f\"random_strings_{size}GB_uppercase.txt\"\n",
        "\n",
        "    with open(file_name, \"r\") as file:\n",
        "        content = file.read().upper()\n",
        "\n",
        "    with open(output_file, \"w\") as file:\n",
        "        file.write(content)\n",
        "\n",
        "    os.remove(file_name)\n"
      ],
      "metadata": {
        "id": "rxHA6oqlIV1p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Converting all the files from Q4 into uppercase in parallel using multi-threading\n",
        "import concurrent.futures\n",
        "\n",
        "file_sizes = [1, 2, 3, 4, 5]  # in GB\n",
        "\n",
        "def convert_to_uppercase(file_name):\n",
        "    output_file = f\"{file_name}_uppercase.txt\"\n",
        "\n",
        "    with open(file_name, \"r\") as file:\n",
        "        content = file.read().upper()\n",
        "\n",
        "    with open(output_file, \"w\") as file:\n",
        "        file.write(content)\n",
        "\n",
        "    os.remove(file_name)\n",
        "\n",
        "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
        "    for size in file_sizes:\n",
        "        file_name = f\"random_strings_{size}GB.txt\"\n",
        "        executor.submit(convert_to_uppercase, file_name)\n"
      ],
      "metadata": {
        "id": "zJrk8qJAInZG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#To automatically download 10 images of cats from Google Images using a package from PyPI\n",
        "from google_images_download import google_images_download\n",
        "\n",
        "response = google_images_download.googleimagesdownload()\n",
        "\n",
        "search_keywords = \"cat\"\n",
        "num_images = 10\n",
        "\n",
        "arguments = {\n",
        "    \"keywords\": search_keywords,\n",
        "    \"limit\": num_images,\n",
        "    \"format\": \"jpg\",\n",
        "    \"output_directory\": \"cat_images\",\n",
        "}\n",
        "\n",
        "response.download(arguments)\n"
      ],
      "metadata": {
        "id": "dMs_cf2sI0cK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#To automatically download 10 videos of \"Machine Learning\" from YouTube using a package from PyPI\n",
        "import pytube\n",
        "\n",
        "search_keywords = \"Machine Learning\"\n",
        "num_videos = 10\n",
        "\n",
        "youtube = pytube.YouTube()\n",
        "results = youtube.search(search_keywords, num_videos)\n",
        "\n",
        "for video in results:\n",
        "    video.streams.get_highest_resolution().download(output_path=\"machine_learning_videos\")\n"
      ],
      "metadata": {
        "id": "MkpOe5psJE4y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#To convert all the videos downloaded in Q8 to audio using a package from PyPI\n",
        "import moviepy.editor as mp\n",
        "\n",
        "video_directory = \"machine_learning_videos\"\n",
        "audio_directory = \"machine_learning_audios\"\n",
        "\n",
        "for video_file in os.listdir(video_directory):\n",
        "    video_path = os.path.join(video_directory, video_file)\n",
        "    video = mp.VideoFileClip(video_path)\n",
        "    audio = video.audio\n",
        "    audio_file = os.path.splitext(video_file)[0] + \".mp3\"\n",
        "    audio_path = os.path.join(audio_directory, audio_file)\n",
        "    audio.write_audiofile(audio_path)\n"
      ],
      "metadata": {
        "id": "6kASNeqPJOQG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating an automated pipeline using multi-threading for automatic downloading of 100 videos from YouTube and converting them to audio\n",
        "import moviepy.editor as mp\n",
        "\n",
        "search_keywords = \"Machine Learning\"\n",
        "num_videos = 100\n",
        "\n",
        "def download_video(video_url, output_directory):\n",
        "    youtube = pytube.YouTube(video_url)\n",
        "    video = youtube.streams.get_highest_resolution()\n",
        "    video.download(output_directory)\n",
        "\n",
        "def convert_to_audio(video_path, audio_directory):\n",
        "    video = mp.VideoFileClip(video_path)\n",
        "    audio = video.audio\n",
        "    audio_file = os.path.splitext(os.path.basename(video_path))[0] + \".mp3\"\n",
        "    audio_path = os.path.join(audio_directory, audio_file)\n",
        "    audio.write_audiofile(audio_path)\n",
        "\n",
        "video_directory = \"machine_learning_videos\"\n",
        "audio_directory = \"machine_learning_audios\"\n",
        "\n",
        "# Download videos in parallel\n",
        "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
        "    youtube = pytube.YouTube()\n",
        "    results = youtube.search(search_keywords, num_videos)\n",
        "    video_urls = [video.watch_url for video in results]\n",
        "    executor.map(download_video, video_urls, [video_directory] * num_videos)\n",
        "\n",
        "# Convert videos to audio in parallel\n",
        "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
        "    for video_file in os.listdir(video_directory):\n",
        "        video_path = os.path.join(video_directory, video_file)\n",
        "        executor.submit(convert_to_audio, video_path, audio_directory)\n"
      ],
      "metadata": {
        "id": "xIJN22pFNdfX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating an automated pipeline using multi-threading for automatically downloading 500 images of dogs from Google Images and rescaling them to 50%\n",
        "from PIL import Image\n",
        "\n",
        "search_keywords = \"dog\"\n",
        "num_images = 500\n",
        "rescale_percentage = 50\n",
        "\n",
        "def rescale_image(image_path, output_directory):\n",
        "    image = Image.open(image_path)\n",
        "    image_rescaled = image.resize((int(image.width * rescale_percentage / 100), int(image.height * rescale_percentage / 100)))\n",
        "    output_path = os.path.join(output_directory, os.path.basename(image_path))\n",
        "    image_rescaled.save(output_path)\n",
        "\n",
        "image_directory = \"dog_images_rescaled\"\n",
        "\n",
        "# Download images from Google Images\n",
        "response = google_images_download.googleimagesdownload()\n",
        "arguments = {\n",
        "    \"keywords\": search_keywords,\n",
        "    \"limit\": num_images,\n",
        "    \"format\": \"jpg\",\n",
        "    \"output_directory\": \"dog_images\",\n",
        "}\n",
        "response.download(arguments)\n",
        "\n",
        "# Rescale images in parallel\n",
        "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
        "    for image_file in os.listdir(\"dog_images\"):\n",
        "        image_path = os.path.join(\"dog_images\", image_file)\n",
        "        executor.submit(rescale_image, image_path, image_directory)\n"
      ],
      "metadata": {
        "id": "4SbKpn62NmXs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#question 12\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Create random dataset\n",
        "dataset = pd.DataFrame(np.random.randint(1, 201, size=(100, 30)))\n",
        "\n",
        "# (i) Replace values with NA in the specified range and count rows with missing values\n",
        "dataset.iloc[10:61] = np.nan\n",
        "missing_rows_count = dataset.isna().any(axis=1).sum()\n",
        "print(f\"Number of rows with missing values: {missing_rows_count}\")\n",
        "\n",
        "# (ii) Replace NA values with column average\n",
        "dataset = dataset.fillna(dataset.mean())\n",
        "\n",
        "# (iii) Calculate Pearson correlation and plot heatmap\n",
        "correlation = dataset.corr()\n",
        "sns.heatmap(correlation, annot=True, cmap=\"coolwarm\")\n",
        "plt.show()\n",
        "\n",
        "# Select columns with correlation <= 0.7\n",
        "selected_columns = correlation.columns[correlation.abs().max() <= 0.7]\n",
        "\n",
        "# (iv) Normalize values between 0 and 10\n",
        "normalized_dataset = (dataset - dataset.min()) / (dataset.max() - dataset.min()) * 10\n",
        "\n",
        "# (v) Replace values with 1 if <= 0.5, else with 0\n",
        "binary_dataset = dataset.applymap(lambda x: 1 if x <= 0.5 else 0)\n"
      ],
      "metadata": {
        "id": "QpFPMlvMNuVf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# question 13\n",
        "from sklearn.cluster import KMeans\n",
        "from scipy.cluster.hierarchy import dendrogram, linkage\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Create random dataset\n",
        "dataset = pd.DataFrame(np.random.uniform(-10, 10, size=(500, 10)), columns=[f\"Column_{i}\" for i in range(1, 11)])\n",
        "\n",
        "# K-Means clustering\n",
        "kmeans = KMeans(n_clusters=3, random_state=42)\n",
        "kmeans.fit(dataset.iloc[:, :8])  # Use columns 1 to 8 for clustering\n",
        "\n",
        "# Determine optimal number of clusters using the Elbow method\n",
        "inertia_values = []\n",
        "for k in range(1, 11):\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
        "    kmeans.fit(dataset.iloc[:, :8])\n",
        "    inertia_values.append(kmeans.inertia_)\n",
        "plt.plot(range(1, 11), inertia_values)\n",
        "plt.xlabel(\"Number of Clusters\")\n",
        "plt.ylabel(\"Inertia\")\n",
        "plt.title(\"Elbow Method\")\n",
        "plt.show()\n",
        "\n",
        "# Hierarchical clustering\n",
        "dendrogram_data = linkage(dataset.iloc[:, 1:5], method='ward')\n",
        "plt.figure(figsize=(10, 6))\n",
        "dendrogram(dendrogram_data, labels=dataset.index, leaf_font_size=8)\n",
        "plt.xlabel(\"Sample Index\")\n",
        "plt.ylabel(\"Distance\")\n",
        "plt.title(\"Hierarchical Clustering Dendrogram\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "l73YdkyGN6iH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#question 14\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Create random dataset\n",
        "dataset = pd.DataFrame(np.random.uniform(-100, 100, size=(600, 15)))\n",
        "\n",
        "# (i) Scatter plot of Column 5 and Column 6\n",
        "plt.scatter(dataset.iloc[:, 4], dataset.iloc[:, 5])\n",
        "plt.xlabel(\"Column 5\")\n",
        "plt.ylabel(\"Column 6\")\n",
        "plt.title(\"Scatter Plot\")\n",
        "plt.show()\n",
        "\n",
        "# (ii) Histogram of each column in a single graph\n",
        "dataset.hist(figsize=(10, 6))\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# (iii) Box plot of each column in a single graph\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.boxplot(data=dataset)\n",
        "plt.xticks(rotation=90)\n",
        "plt.title(\"Box Plot\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "RtpMS2BcOBYr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#question 15\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.stats import ttest_1samp, wilcoxon, ttest_ind, ranksums\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Create random dataset\n",
        "dataset = pd.DataFrame(np.random.uniform(5, 10, size=(500, 5)), columns=[f\"Column_{i}\" for i in range(1, 6)])\n",
        "\n",
        "# (i) Perform t-Test on each column\n",
        "t_test_results = {}\n",
        "for column in dataset.columns:\n",
        "    t_statistic, p_value = ttest_1samp(dataset[column], 5)\n",
        "    t_test_results[column] = {\"t-statistic\": t_statistic, \"p-value\": p_value}\n",
        "\n",
        "# (ii) Perform Wilcoxon Signed Rank Test on each column\n",
        "wilcoxon_results = {}\n",
        "for column in dataset.columns:\n",
        "    statistic, p_value = wilcoxon(dataset[column] - 5)\n",
        "    wilcoxon_results[column] = {\"Statistic\": statistic, \"p-value\": p_value}\n",
        "\n",
        "# (iii) Perform Two Sample t-Test and Wilcoxon Rank Sum Test on Column 3 and Column 4\n",
        "column_3 = dataset[\"Column_3\"]\n",
        "column_4 = dataset[\"Column_4\"]\n",
        "t_test_2samp_result = ttest_ind(column_3, column_4)\n",
        "wilcoxon_ranksums_result = ranksums(column_3, column_4)\n"
      ],
      "metadata": {
        "id": "2j2qIF6ROHUc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}